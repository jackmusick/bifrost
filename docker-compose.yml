name: bifrost

# Bifrost Docker Stack - Production Configuration
#
# Usage:
#   docker compose up              # Start all services
#   docker compose up -d           # Start in background
#   docker compose logs -f api     # Follow API logs
#
# For development with hot reload:
#   docker compose -f docker-compose.yml -f docker-compose.dev.yml up
#
# Required environment variables:
#   BIFROST_SECRET_KEY  - 32+ char secret for JWT signing and secret encryption
#   POSTGRES_PASSWORD   - PostgreSQL password
#   RABBITMQ_PASSWORD   - RabbitMQ password
#
# The BIFROST_SECRET_KEY is critical - it's used for:
#   - JWT token signing (HS256)
#   - Fernet encryption of secrets stored in PostgreSQL
#   - Must be consistent across API and Jobs workers
#   - Changing it will invalidate all existing tokens and secrets
#
# S3/MinIO Configuration:
#   - Local/self-hosted can use MinIO for S3-compatible storage
#   - Production can use AWS S3, Azure Blob, or any S3-compatible storage
#   - Workers download workspace from S3 before execution (enables horizontal scaling)

services:
  # PostgreSQL Database (with pgvector extension for semantic search)
  postgres:
    image: pgvector/pgvector:pg16
    container_name: bifrost-postgres
    environment:
      POSTGRES_DB: bifrost
      POSTGRES_USER: bifrost
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD is required}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U bifrost -d bifrost"]
      interval: 5s
      timeout: 5s
      retries: 5

  # RabbitMQ Message Broker
  rabbitmq:
    image: rabbitmq:3.13-management-alpine
    container_name: bifrost-rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: bifrost
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:?RABBITMQ_PASSWORD is required}
    ports:
      - "5672:5672"
      # Management UI port (15672) only exposed in docker-compose.dev.yml
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
      - ./config/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_running"]
      interval: 10s
      timeout: 10s
      retries: 5

  # Redis (Sessions & Cache)
  redis:
    image: redis:7-alpine
    container_name: bifrost-redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  # MinIO - S3-compatible Object Storage
  # Used for workspace files (enables horizontal scaling of workers)
  minio:
    image: minio/minio:latest
    container_name: bifrost-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-bifrost}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:?MINIO_ROOT_PASSWORD is required}
    ports:
      - "9000:9000"
      # Console port (9001) only exposed in docker-compose.dev.yml
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5

  # MinIO Initialization - Creates required buckets
  minio-init:
    image: minio/mc:latest
    container_name: bifrost-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 $${MINIO_ROOT_USER:-bifrost} $${MINIO_ROOT_PASSWORD};
      mc mb myminio/bifrost-local --ignore-existing;
      echo 'Bucket created successfully';
      "
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-bifrost}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:?MINIO_ROOT_PASSWORD is required}

  # Init Container - Runs migrations and warms module cache before services start
  init:
    # build:
    #   context: .
    #   dockerfile: api/Dockerfile
    image: jackmusick/bifrost-api:latest
    container_name: bifrost-init
    environment:
      # Core (required by Settings during migrations)
      BIFROST_ENVIRONMENT: ${BIFROST_ENVIRONMENT:-production}
      BIFROST_SECRET_KEY: ${BIFROST_SECRET_KEY:?BIFROST_SECRET_KEY is required (32+ chars)}
      # Database (via PgBouncer for connection pooling)
      BIFROST_DATABASE_URL: postgresql+asyncpg://bifrost:${POSTGRES_PASSWORD}@pgbouncer:5432/bifrost
      BIFROST_DATABASE_URL_SYNC: postgresql://bifrost:${POSTGRES_PASSWORD}@pgbouncer:5432/bifrost
      # Redis (for module cache warming)
      BIFROST_REDIS_URL: redis://redis:6379/0
    depends_on:
      pgbouncer:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: python -m scripts.init_container

  # PgBouncer - Connection Pooler for PostgreSQL
  # All services connect through PgBouncer for centralized connection management
  # Transaction mode: connections returned after each transaction (optimal for short-lived queries)
  pgbouncer:
    image: edoburu/pgbouncer:latest
    container_name: bifrost-pgbouncer
    environment:
      # Database connection
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: bifrost
      DB_USER: bifrost
      DB_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD is required}
      # PgBouncer settings
      POOL_MODE: transaction
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 20
      MIN_POOL_SIZE: 5
      RESERVE_POOL_SIZE: 5
      # Auth
      AUTH_TYPE: plain
    ports:
      - "6432:5432" # pgbouncer listens on 5432 internally
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test:
        ["CMD", "pg_isready", "-h", "localhost", "-p", "5432", "-U", "bifrost"]
      interval: 5s
      timeout: 5s
      retries: 5

  # FastAPI Application
  api:
    # build:
    #   context: .
    #   dockerfile: api/Dockerfile
    image: jackmusick/bifrost-api:latest
    environment:
      # Core
      BIFROST_ENVIRONMENT: ${BIFROST_ENVIRONMENT:-production}
      BIFROST_DEBUG: ${BIFROST_DEBUG:-false}
      BIFROST_SECRET_KEY: ${BIFROST_SECRET_KEY:?BIFROST_SECRET_KEY is required (32+ chars)}
      # Database (via PgBouncer for connection pooling)
      BIFROST_DATABASE_URL: postgresql+asyncpg://bifrost:${POSTGRES_PASSWORD}@pgbouncer:5432/bifrost
      BIFROST_DATABASE_URL_SYNC: postgresql://bifrost:${POSTGRES_PASSWORD}@pgbouncer:5432/bifrost
      # Message Queue & Cache
      BIFROST_RABBITMQ_URL: amqp://bifrost:${RABBITMQ_PASSWORD}@rabbitmq:5672/
      BIFROST_REDIS_URL: redis://redis:6379/0
      # S3 Storage (MinIO locally, AWS S3 in production)
      BIFROST_S3_BUCKET: ${BIFROST_S3_BUCKET:-bifrost-local}
      BIFROST_S3_ENDPOINT_URL: ${BIFROST_S3_ENDPOINT_URL:-http://minio:9000}
      BIFROST_S3_ACCESS_KEY: ${BIFROST_S3_ACCESS_KEY:-${MINIO_ROOT_USER:-bifrost}}
      BIFROST_S3_SECRET_KEY: ${BIFROST_S3_SECRET_KEY:-${MINIO_ROOT_PASSWORD}}
      BIFROST_S3_REGION: ${BIFROST_S3_REGION:-us-east-1}
      # CORS (only needed if API is accessed directly, not through proxy)
      BIFROST_CORS_ORIGINS: ${BIFROST_CORS_ORIGINS:-http://localhost:3000}
      # JWT (optional, has defaults)
      BIFROST_ACCESS_TOKEN_EXPIRE_MINUTES: ${BIFROST_ACCESS_TOKEN_EXPIRE_MINUTES:-30}
      BIFROST_REFRESH_TOKEN_EXPIRE_DAYS: ${BIFROST_REFRESH_TOKEN_EXPIRE_DAYS:-7}
      # MFA (optional, has defaults)
      BIFROST_MFA_ENABLED: ${BIFROST_MFA_ENABLED:-true}
      BIFROST_MFA_TOTP_ISSUER: ${BIFROST_MFA_TOTP_ISSUER:-Bifrost}
      # WebAuthn/Passkeys (comma-separated for multiple origins, e.g., localhost + ngrok)
      BIFROST_WEBAUTHN_ORIGIN: ${BIFROST_WEBAUTHN_ORIGIN:-http://localhost:3000}
      # Default admin user (optional - set in .env to skip setup wizard)
      BIFROST_DEFAULT_USER_EMAIL: ${BIFROST_DEFAULT_USER_EMAIL:-}
      BIFROST_DEFAULT_USER_PASSWORD: ${BIFROST_DEFAULT_USER_PASSWORD:-}
      # MCP - Set this to your public address (https://bifrost.yourdomain.com)
      BIFROST_MCP_BASE_URL: ${BIFROST_MCP_BASE_URL:-}
    # Note: API port 8000 not exposed - access via client proxy at http://localhost:3000
    # Debug port (5678) only exposed in docker-compose.dev.yml
    depends_on:
      init:
        condition: service_completed_successfully
      pgbouncer:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      replicas: ${API_REPLICAS:-1}
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000

  # Frontend (Vite)
  client:
    # build:
    #   context: ./client
    #   dockerfile: Dockerfile
    #   target: production
    image: jackmusick/bifrost-client:latest
    container_name: bifrost-client
    environment:
      API_URL: http://api:8000
      WS_URL: ws://api:8000
    ports:
      - "3000:80"
    depends_on:
      api:
        condition: service_healthy

  # Scheduler Service (APScheduler only)
  # MUST run as single instance - handles CRON jobs, cleanup tasks, OAuth refresh
  scheduler:
    # build:
    #   context: .
    #   dockerfile: api/Dockerfile
    image: jackmusick/bifrost-api:latest
    container_name: bifrost-scheduler
    environment:
      # Core (must match API)
      BIFROST_ENVIRONMENT: ${BIFROST_ENVIRONMENT:-production}
      BIFROST_SECRET_KEY: ${BIFROST_SECRET_KEY:?BIFROST_SECRET_KEY is required (32+ chars)}
      # Database (via PgBouncer for connection pooling)
      BIFROST_DATABASE_URL: postgresql+asyncpg://bifrost:${POSTGRES_PASSWORD}@pgbouncer:5432/bifrost
      BIFROST_DATABASE_URL_SYNC: postgresql://bifrost:${POSTGRES_PASSWORD}@pgbouncer:5432/bifrost
      # Message Queue & Cache
      BIFROST_RABBITMQ_URL: amqp://bifrost:${RABBITMQ_PASSWORD}@rabbitmq:5672/
      BIFROST_REDIS_URL: redis://redis:6379/0
      # API URL (for SDK client injection in workflows)
      BIFROST_API_URL: http://api:8000
      # S3 Storage (for scheduled jobs that need file access)
      BIFROST_S3_BUCKET: ${BIFROST_S3_BUCKET:-bifrost-local}
      BIFROST_S3_ENDPOINT_URL: ${BIFROST_S3_ENDPOINT_URL:-http://minio:9000}
      BIFROST_S3_ACCESS_KEY: ${BIFROST_S3_ACCESS_KEY:-${MINIO_ROOT_USER:-bifrost}}
      BIFROST_S3_SECRET_KEY: ${BIFROST_S3_SECRET_KEY:-${MINIO_ROOT_PASSWORD}}
      BIFROST_S3_REGION: ${BIFROST_S3_REGION:-us-east-1}
    depends_on:
      pgbouncer:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    deploy:
      replicas: 1 # MUST be single instance
    command: >
      python -m src.scheduler.main

  # Worker Service (RabbitMQ Consumers)
  # Can be scaled horizontally for increased throughput
  # Workers load Python modules from Redis cache (no filesystem sync needed)
  worker:
    # build:
    #   context: .
    #   dockerfile: api/Dockerfile
    image: jackmusick/bifrost-api:latest
    environment:
      # Core (must match API)
      BIFROST_ENVIRONMENT: ${BIFROST_ENVIRONMENT:-production}
      BIFROST_SECRET_KEY: ${BIFROST_SECRET_KEY:?BIFROST_SECRET_KEY is required (32+ chars)}
      # Database (via PgBouncer for connection pooling)
      BIFROST_DATABASE_URL: postgresql+asyncpg://bifrost:${POSTGRES_PASSWORD}@pgbouncer:5432/bifrost
      BIFROST_DATABASE_URL_SYNC: postgresql://bifrost:${POSTGRES_PASSWORD}@pgbouncer:5432/bifrost
      # Message Queue & Cache
      BIFROST_RABBITMQ_URL: amqp://bifrost:${RABBITMQ_PASSWORD}@rabbitmq:5672/
      BIFROST_REDIS_URL: redis://redis:6379/0
      # API URL (for SDK client injection in workflows)
      BIFROST_API_URL: http://api:8000
      # S3 Storage (for non-Python file access)
      BIFROST_S3_BUCKET: ${BIFROST_S3_BUCKET:-bifrost-local}
      BIFROST_S3_ENDPOINT_URL: ${BIFROST_S3_ENDPOINT_URL:-http://minio:9000}
      BIFROST_S3_ACCESS_KEY: ${BIFROST_S3_ACCESS_KEY:-${MINIO_ROOT_USER:-bifrost}}
      BIFROST_S3_SECRET_KEY: ${BIFROST_S3_SECRET_KEY:-${MINIO_ROOT_PASSWORD}}
      BIFROST_S3_REGION: ${BIFROST_S3_REGION:-us-east-1}
      # Concurrency
      BIFROST_MAX_CONCURRENCY: ${BIFROST_MAX_CONCURRENCY:-10}
    # No workspace volume mount needed - modules loaded from Redis cache
    depends_on:
      init:
        condition: service_completed_successfully
      pgbouncer:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    deploy:
      replicas: ${WORKER_REPLICAS:-1}
    command: python -m src.worker.main

  # Coding Agent Service (Claude Agent SDK)
  # Runs SDK in isolation - can block without affecting API health
  # MUST run as single instance (SDK clients cached per session)
  coding-agent:
    # build:
    #   context: .
    #   dockerfile: api/Dockerfile
    image: jackmusick/bifrost-api:latest
    container_name: bifrost-coding-agent
    environment:
      # Core (must match API)
      BIFROST_ENVIRONMENT: ${BIFROST_ENVIRONMENT:-production}
      BIFROST_SECRET_KEY: ${BIFROST_SECRET_KEY:?BIFROST_SECRET_KEY is required (32+ chars)}
      # Database (via PgBouncer for connection pooling)
      BIFROST_DATABASE_URL: postgresql+asyncpg://bifrost:${POSTGRES_PASSWORD}@pgbouncer:5432/bifrost
      BIFROST_DATABASE_URL_SYNC: postgresql://bifrost:${POSTGRES_PASSWORD}@pgbouncer:5432/bifrost
      # Message Queue & Cache
      BIFROST_RABBITMQ_URL: amqp://bifrost:${RABBITMQ_PASSWORD}@rabbitmq:5672/
      BIFROST_REDIS_URL: redis://redis:6379/0
      # API URL (for MCP tools that call back to API)
      BIFROST_API_URL: http://api:8000
      # S3 Storage (coding agent syncs workspace from S3)
      BIFROST_S3_BUCKET: ${BIFROST_S3_BUCKET:-bifrost-local}
      BIFROST_S3_ENDPOINT_URL: ${BIFROST_S3_ENDPOINT_URL:-http://minio:9000}
      BIFROST_S3_ACCESS_KEY: ${BIFROST_S3_ACCESS_KEY:-${MINIO_ROOT_USER:-bifrost}}
      BIFROST_S3_SECRET_KEY: ${BIFROST_S3_SECRET_KEY:-${MINIO_ROOT_PASSWORD}}
      BIFROST_S3_REGION: ${BIFROST_S3_REGION:-us-east-1}
      # Anthropic API key for Claude Agent SDK
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
    # No liveness probe - SDK can block for extended periods
    depends_on:
      pgbouncer:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    deploy:
      replicas: 1 # Single instance - SDK clients cached per session
    command: >
      python -m src.coding_agent.main

volumes:
  postgres_data:
  rabbitmq_data:
  redis_data:
  minio_data:

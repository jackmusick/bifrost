# Bifrost Docker Stack - Full Development Environment
#
# Usage:
#   docker compose up              # Start all services with hot reload
#   docker compose up -d           # Start in background
#   docker compose logs -f api     # Follow API logs
#
# Debugging with VS Code:
#   ENABLE_DEBUG=true docker compose up
#   Then attach VS Code debugger to port 5678 (API waits for debugger)
#
# Required environment variables for production:
#   BIFROST_SECRET_KEY  - 32+ char secret for JWT signing and secret encryption
#   POSTGRES_PASSWORD   - PostgreSQL password
#   RABBITMQ_PASSWORD   - RabbitMQ password
#
# The BIFROST_SECRET_KEY is critical - it's used for:
#   - JWT token signing (HS256)
#   - Fernet encryption of secrets stored in PostgreSQL
#   - Must be consistent across API and Jobs workers
#   - Changing it will invalidate all existing tokens and secrets
#
# S3/MinIO Configuration:
#   - Local dev uses MinIO for S3-compatible storage
#   - Production can use AWS S3, Azure Blob, or any S3-compatible storage
#   - Workers download workspace from S3 before execution (enables horizontal scaling)

services:
  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    container_name: bifrost-postgres
    environment:
      POSTGRES_DB: bifrost
      POSTGRES_USER: bifrost
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-bifrost_dev}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U bifrost -d bifrost"]
      interval: 5s
      timeout: 5s
      retries: 5

  # RabbitMQ Message Broker
  rabbitmq:
    image: rabbitmq:3.13-management-alpine
    container_name: bifrost-rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: bifrost
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:-bifrost_dev}
    ports:
      - "5672:5672" # AMQP
      - "15672:15672" # Management UI (for debugging only)
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
      - ./config/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_running"]
      interval: 10s
      timeout: 10s
      retries: 5

  # Redis (Sessions & Cache)
  redis:
    image: redis:7-alpine
    container_name: bifrost-redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  # MinIO - S3-compatible Object Storage
  # Used for workspace files (enables horizontal scaling of workers)
  minio:
    image: minio/minio:latest
    container_name: bifrost-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-bifrost}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-bifrost123}
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # Web Console
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5

  # MinIO Initialization - Creates required buckets
  minio-init:
    image: minio/mc:latest
    container_name: bifrost-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 $${MINIO_ROOT_USER:-bifrost} $${MINIO_ROOT_PASSWORD:-bifrost123};
      mc mb myminio/bifrost-local-workspace --ignore-existing;
      mc mb myminio/bifrost-local-files --ignore-existing;
      echo 'Buckets created successfully';
      "
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-bifrost}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-bifrost123}

  # PgBouncer - Connection Pooler for PostgreSQL
  # All services connect through PgBouncer for centralized connection management
  # Transaction mode: connections returned after each transaction (optimal for short-lived queries)
  pgbouncer:
    image: edoburu/pgbouncer:latest
    container_name: bifrost-pgbouncer
    environment:
      # Database connection
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: bifrost
      DB_USER: bifrost
      DB_PASSWORD: ${POSTGRES_PASSWORD:-bifrost_dev}
      # PgBouncer settings
      POOL_MODE: transaction
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 20
      MIN_POOL_SIZE: 5
      RESERVE_POOL_SIZE: 5
      # Auth
      AUTH_TYPE: plain
    ports:
      - "6432:5432"  # pgbouncer listens on 5432 internally
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "pg_isready", "-h", "localhost", "-p", "5432", "-U", "bifrost"]
      interval: 5s
      timeout: 5s
      retries: 5

  # FastAPI Application
  api:
    build:
      context: .
      dockerfile: api/Dockerfile
    container_name: bifrost-api
    environment:
      # Core
      BIFROST_ENVIRONMENT: ${BIFROST_ENVIRONMENT:-development}
      BIFROST_DEBUG: ${BIFROST_DEBUG:-false}
      BIFROST_SECRET_KEY: ${BIFROST_SECRET_KEY:-dev-secret-key-change-in-production-must-be-32-chars}
      # Database (via PgBouncer for connection pooling)
      BIFROST_DATABASE_URL: postgresql+asyncpg://bifrost:${POSTGRES_PASSWORD:-bifrost_dev}@pgbouncer:5432/bifrost
      BIFROST_DATABASE_URL_SYNC: postgresql://bifrost:${POSTGRES_PASSWORD:-bifrost_dev}@pgbouncer:5432/bifrost
      # Message Queue & Cache
      BIFROST_RABBITMQ_URL: amqp://bifrost:${RABBITMQ_PASSWORD:-bifrost_dev}@rabbitmq:5672/
      BIFROST_REDIS_URL: redis://redis:6379/0
      # S3 Storage (MinIO in dev, AWS S3 in production)
      BIFROST_INSTANCE_ID: ${BIFROST_INSTANCE_ID:-local}
      BIFROST_S3_ENDPOINT_URL: http://minio:9000
      BIFROST_S3_ACCESS_KEY: ${MINIO_ROOT_USER:-bifrost}
      BIFROST_S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-bifrost123}
      BIFROST_S3_REGION: us-east-1
      # CORS & Frontend
      BIFROST_CORS_ORIGINS: ${BIFROST_CORS_ORIGINS:-http://localhost:3000}
      BIFROST_FRONTEND_URL: ${BIFROST_FRONTEND_URL:-http://localhost:3000}
      # JWT (optional, has defaults)
      BIFROST_ACCESS_TOKEN_EXPIRE_MINUTES: ${BIFROST_ACCESS_TOKEN_EXPIRE_MINUTES:-30}
      BIFROST_REFRESH_TOKEN_EXPIRE_DAYS: ${BIFROST_REFRESH_TOKEN_EXPIRE_DAYS:-7}
      # MFA (optional, has defaults)
      BIFROST_MFA_ENABLED: ${BIFROST_MFA_ENABLED:-true}
      BIFROST_MFA_TOTP_ISSUER: ${BIFROST_MFA_TOTP_ISSUER:-Bifrost}
      # Default admin user (optional - set in .env to skip setup wizard)
      BIFROST_DEFAULT_USER_EMAIL: ${BIFROST_DEFAULT_USER_EMAIL:-}
      BIFROST_DEFAULT_USER_PASSWORD: ${BIFROST_DEFAULT_USER_PASSWORD:-}
      # OAuth SSO (optional - set in .env to enable)
      BIFROST_MICROSOFT_CLIENT_ID: ${BIFROST_MICROSOFT_CLIENT_ID:-}
      BIFROST_MICROSOFT_CLIENT_SECRET: ${BIFROST_MICROSOFT_CLIENT_SECRET:-}
      BIFROST_MICROSOFT_TENANT_ID: ${BIFROST_MICROSOFT_TENANT_ID:-}
      BIFROST_GOOGLE_CLIENT_ID: ${BIFROST_GOOGLE_CLIENT_ID:-}
      BIFROST_GOOGLE_CLIENT_SECRET: ${BIFROST_GOOGLE_CLIENT_SECRET:-}
      BIFROST_OIDC_DISCOVERY_URL: ${BIFROST_OIDC_DISCOVERY_URL:-}
      BIFROST_OIDC_CLIENT_ID: ${BIFROST_OIDC_CLIENT_ID:-}
      BIFROST_OIDC_CLIENT_SECRET: ${BIFROST_OIDC_CLIENT_SECRET:-}
      # Debugging (optional - enables debugpy)
      ENABLE_DEBUG: ${ENABLE_DEBUG:-false}
    ports:
      - "5678:5678" # debugpy port for VS Code attachment
    # Note: API port 8000 not exposed - access via client proxy at http://localhost:3000
    # No workspace volume mount needed - files stored in S3
    depends_on:
      pgbouncer:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      replicas: ${API_REPLICAS:-1}
    command: >
      sh -c "alembic upgrade head &&
             uvicorn src.main:app --host 0.0.0.0 --port 8000"

  # Frontend (Vite)
  client:
    build:
      context: ./client
      dockerfile: Dockerfile
      target: production
    container_name: bifrost-client
    environment:
      API_URL: http://api:8000
      WS_URL: ws://api:8000
    ports:
      - "3000:3000"
    depends_on:
      api:
        condition: service_healthy

  # NOTE: Discovery service has been removed!
  # Workflow/form metadata extraction now happens at write time in FileStorageService.
  # This eliminates the need for file watching and enables horizontal scaling.

  # Scheduler Service (APScheduler only)
  # MUST run as single instance - handles CRON jobs, cleanup tasks, OAuth refresh
  scheduler:
    build:
      context: .
      dockerfile: api/Dockerfile
    container_name: bifrost-scheduler
    environment:
      # Core (must match API)
      BIFROST_ENVIRONMENT: ${BIFROST_ENVIRONMENT:-development}
      BIFROST_SECRET_KEY: ${BIFROST_SECRET_KEY:-dev-secret-key-change-in-production-must-be-32-chars}
      # Database (via PgBouncer for connection pooling)
      BIFROST_DATABASE_URL: postgresql+asyncpg://bifrost:${POSTGRES_PASSWORD:-bifrost_dev}@pgbouncer:5432/bifrost
      BIFROST_DATABASE_URL_SYNC: postgresql://bifrost:${POSTGRES_PASSWORD:-bifrost_dev}@pgbouncer:5432/bifrost
      # Message Queue & Cache
      BIFROST_RABBITMQ_URL: amqp://bifrost:${RABBITMQ_PASSWORD:-bifrost_dev}@rabbitmq:5672/
      BIFROST_REDIS_URL: redis://redis:6379/0
      # S3 Storage (for scheduled jobs that need file access)
      BIFROST_INSTANCE_ID: ${BIFROST_INSTANCE_ID:-local}
      BIFROST_S3_ENDPOINT_URL: http://minio:9000
      BIFROST_S3_ACCESS_KEY: ${MINIO_ROOT_USER:-bifrost}
      BIFROST_S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-bifrost123}
      BIFROST_S3_REGION: us-east-1
    depends_on:
      pgbouncer:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    deploy:
      replicas: 1 # MUST be single instance
    command: >
      python -m src.scheduler.main

  # Worker Service (RabbitMQ Consumers)
  # Can be scaled horizontally for increased throughput
  # Workers download workspace from S3 before execution
  worker:
    build:
      context: .
      dockerfile: api/Dockerfile
    container_name: bifrost-worker
    environment:
      # Core (must match API)
      BIFROST_ENVIRONMENT: ${BIFROST_ENVIRONMENT:-development}
      BIFROST_SECRET_KEY: ${BIFROST_SECRET_KEY:-dev-secret-key-change-in-production-must-be-32-chars}
      # Database (via PgBouncer for connection pooling)
      BIFROST_DATABASE_URL: postgresql+asyncpg://bifrost:${POSTGRES_PASSWORD:-bifrost_dev}@pgbouncer:5432/bifrost
      BIFROST_DATABASE_URL_SYNC: postgresql://bifrost:${POSTGRES_PASSWORD:-bifrost_dev}@pgbouncer:5432/bifrost
      # Message Queue & Cache
      BIFROST_RABBITMQ_URL: amqp://bifrost:${RABBITMQ_PASSWORD:-bifrost_dev}@rabbitmq:5672/
      BIFROST_REDIS_URL: redis://redis:6379/0
      # S3 Storage (workers download workspace from S3)
      BIFROST_INSTANCE_ID: ${BIFROST_INSTANCE_ID:-local}
      BIFROST_S3_ENDPOINT_URL: http://minio:9000
      BIFROST_S3_ACCESS_KEY: ${MINIO_ROOT_USER:-bifrost}
      BIFROST_S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-bifrost123}
      BIFROST_S3_REGION: us-east-1
      # Concurrency
      BIFROST_MAX_CONCURRENCY: ${BIFROST_MAX_CONCURRENCY:-10}
    # No workspace volume mount needed - files downloaded from S3
    depends_on:
      pgbouncer:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    deploy:
      replicas: ${WORKER_REPLICAS:-2}
    command: >
      python -m src.worker.main

volumes:
  postgres_data:
  rabbitmq_data:
  redis_data:
  minio_data:

"""
CLI Router

Endpoints for the Bifrost CLI:
- Developer context (default organization, parameters)
- CLI package download
- Config operations (get, set, list, delete)
- CLI Sessions (register, state, continue, pending, log, result)

Note: File operations have been moved to /api/files router.
"""

import asyncio
import io
import json
import logging
import tarfile
from datetime import datetime
from pathlib import Path
from typing import Any
from uuid import UUID, uuid4

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from src.core.auth import CurrentUser
from src.core.database import get_db
from src.models import DeveloperContext, Organization
from src.models.contracts.cli import (
    CLIAICompleteRequest,
    CLIAICompleteResponse,
    CLIAIInfoResponse,
    CLIConfigDeleteRequest,
    CLIConfigGetRequest,
    CLIConfigListRequest,
    CLIConfigSetRequest,
    CLIConfigValue,
    CLIKnowledgeDeleteRequest,
    CLIKnowledgeDocumentResponse,
    CLIKnowledgeNamespaceInfo,
    CLIKnowledgeSearchRequest,
    CLIKnowledgeStoreManyRequest,
    CLIKnowledgeStoreRequest,
    CLIRegisteredWorkflow,
    CLISessionContinueRequest,
    CLISessionContinueResponse,
    CLISessionExecutionSummary,
    CLISessionListResponse,
    CLISessionLogRequest,
    CLISessionPendingResponse,
    CLISessionRegisterRequest,
    CLISessionResponse,
    CLISessionResultRequest,
    SDKIntegrationsGetRequest,
    SDKIntegrationsGetResponse,
    SDKIntegrationsOAuthData,
    SDKIntegrationsListMappingsRequest,
    SDKIntegrationsListMappingsResponse,
)
from src.core.cache import config_hash_key, get_redis
from src.core.pubsub import publish_cli_session_update, publish_execution_log, publish_execution_update
from src.repositories.cli_sessions import CLISessionRepository

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/cli", tags=["CLI"])


# =============================================================================
# Pydantic Models (Developer Context)
# =============================================================================


class DeveloperContextResponse(BaseModel):
    """Developer context for CLI initialization."""

    user: dict = Field(description="User information")
    organization: dict | None = Field(description="Default organization")
    default_parameters: dict = Field(default={}, description="Default workflow parameters")
    track_executions: bool = Field(default=True, description="Whether to track executions in history")


class DeveloperContextUpdate(BaseModel):
    """Update developer context settings."""

    default_org_id: UUID | None = Field(default=None, description="Default organization ID")
    default_parameters: dict | None = Field(default=None, description="Default workflow parameters")
    track_executions: bool | None = Field(default=None, description="Track executions in history")


# =============================================================================
# Helper Functions
# =============================================================================


def _session_to_response(
    session,
    is_connected: bool,
) -> CLISessionResponse:
    """Convert CLISession ORM to response model."""
    from sqlalchemy import inspect as sa_inspect

    executions = []
    # Use SQLAlchemy inspect to check if executions were eagerly loaded
    # This avoids triggering a lazy load which would fail in async context
    state = sa_inspect(session)
    if 'executions' in state.dict and state.dict['executions']:
        for ex in sorted(state.dict['executions'], key=lambda e: e.created_at, reverse=True)[:10]:
            executions.append(CLISessionExecutionSummary(
                id=str(ex.id),
                workflow_name=ex.workflow_name,
                status=ex.status.value if hasattr(ex.status, 'value') else str(ex.status),
                created_at=ex.created_at,
                duration_ms=ex.duration_ms,
            ))

    workflows = []
    if session.workflows:
        for w in session.workflows:
            workflows.append(CLIRegisteredWorkflow(
                name=w.get("name", ""),
                description=w.get("description", ""),
                parameters=w.get("parameters", []),
            ))

    return CLISessionResponse(
        id=str(session.id),
        user_id=str(session.user_id),
        file_path=session.file_path,
        workflows=workflows,
        selected_workflow=session.selected_workflow,
        params=session.params,
        pending=session.pending,
        last_seen=session.last_seen,
        created_at=session.created_at,
        is_connected=is_connected,
        executions=executions,
    )


# =============================================================================
# Context Endpoints
# =============================================================================


@router.get(
    "/context",
    response_model=DeveloperContextResponse,
    summary="Get developer context",
)
async def get_dev_context(
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> DeveloperContextResponse:
    """Get development context for CLI initialization."""
    stmt = select(DeveloperContext).where(DeveloperContext.user_id == current_user.user_id)
    result = await db.execute(stmt)
    dev_ctx = result.scalar_one_or_none()

    org_data = None
    if dev_ctx and dev_ctx.default_org_id:
        stmt = select(Organization).where(Organization.id == dev_ctx.default_org_id)
        result = await db.execute(stmt)
        org = result.scalar_one_or_none()
        if org:
            org_data = {
                "id": str(org.id),
                "name": org.name,
                "is_active": org.is_active,
            }

    return DeveloperContextResponse(
        user={
            "id": str(current_user.user_id),
            "email": current_user.email,
            "name": current_user.name,
        },
        organization=org_data,
        default_parameters=dev_ctx.default_parameters if dev_ctx else {},
        track_executions=dev_ctx.track_executions if dev_ctx else True,
    )


@router.put(
    "/context",
    response_model=DeveloperContextResponse,
    summary="Update developer context",
)
async def update_dev_context(
    request: DeveloperContextUpdate,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> DeveloperContextResponse:
    """Update developer context settings."""
    stmt = select(DeveloperContext).where(DeveloperContext.user_id == current_user.user_id)
    result = await db.execute(stmt)
    dev_ctx = result.scalar_one_or_none()

    if not dev_ctx:
        dev_ctx = DeveloperContext(
            user_id=current_user.user_id,
            default_org_id=request.default_org_id,
            default_parameters=request.default_parameters or {},
            track_executions=request.track_executions if request.track_executions is not None else True,
        )
        db.add(dev_ctx)
    else:
        if request.default_org_id is not None:
            dev_ctx.default_org_id = request.default_org_id
        if request.default_parameters is not None:
            dev_ctx.default_parameters = request.default_parameters
        if request.track_executions is not None:
            dev_ctx.track_executions = request.track_executions

    await db.commit()
    await db.refresh(dev_ctx)

    org_data = None
    if dev_ctx.default_org_id:
        stmt = select(Organization).where(Organization.id == dev_ctx.default_org_id)
        result = await db.execute(stmt)
        org = result.scalar_one_or_none()
        if org:
            org_data = {
                "id": str(org.id),
                "name": org.name,
                "is_active": org.is_active,
            }

    return DeveloperContextResponse(
        user={
            "id": str(current_user.user_id),
            "email": current_user.email,
            "name": current_user.name,
        },
        organization=org_data,
        default_parameters=dev_ctx.default_parameters,
        track_executions=dev_ctx.track_executions,
    )


# =============================================================================
# CLI Config Operations
# =============================================================================


async def _get_cli_org_id(
    user_id: UUID,
    requested_org_id: str | None,
    db: AsyncSession,
) -> str | None:
    """Get the organization ID for CLI config operations."""
    if requested_org_id:
        return requested_org_id

    stmt = select(DeveloperContext).where(DeveloperContext.user_id == user_id)
    result = await db.execute(stmt)
    dev_ctx = result.scalar_one_or_none()

    if dev_ctx and dev_ctx.default_org_id:
        return str(dev_ctx.default_org_id)

    return None


@router.post(
    "/config/get",
    response_model=CLIConfigValue | None,
    summary="Get config value",
)
async def cli_get_config(
    request: CLIConfigGetRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> CLIConfigValue | None:
    """Get a config value via CLI API."""
    org_id = await _get_cli_org_id(current_user.user_id, request.org_id, db)

    async with get_redis() as r:
        data = await r.hget(config_hash_key(org_id), request.key)  # type: ignore[misc]

        if data is None:
            return None

        try:
            cache_entry = json.loads(data)
        except json.JSONDecodeError:
            return None

        raw_value = cache_entry.get("value")
        config_type = cache_entry.get("type", "string")

        if config_type == "json" and isinstance(raw_value, str):
            try:
                raw_value = json.loads(raw_value)
            except json.JSONDecodeError:
                pass
        elif config_type == "bool":
            raw_value = str(raw_value).lower() == "true" if isinstance(raw_value, str) else bool(raw_value)
        elif config_type == "int":
            try:
                raw_value = int(raw_value)
            except (ValueError, TypeError):
                pass

        return CLIConfigValue(
            key=request.key,
            value=raw_value,
            config_type=config_type,
        )


@router.post(
    "/config/set",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Set config value",
)
async def cli_set_config(
    request: CLIConfigSetRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> None:
    """Set a config value via CLI API."""
    from src.models import Config as ConfigModel
    from src.models.enums import ConfigType as ConfigTypeEnum

    org_id = await _get_cli_org_id(current_user.user_id, request.org_id, db)
    org_uuid = UUID(org_id) if org_id else None
    now = datetime.utcnow()

    if request.is_secret:
        from src.core.security import encrypt_secret
        config_type = ConfigTypeEnum.SECRET
        stored_value = await asyncio.to_thread(encrypt_secret, str(request.value))
    elif isinstance(request.value, dict) or isinstance(request.value, list):
        config_type = ConfigTypeEnum.JSON
        stored_value = request.value
    elif isinstance(request.value, bool):
        config_type = ConfigTypeEnum.BOOL
        stored_value = request.value
    elif isinstance(request.value, int):
        config_type = ConfigTypeEnum.INT
        stored_value = request.value
    else:
        config_type = ConfigTypeEnum.STRING
        stored_value = request.value

    config_value = {"value": stored_value}

    stmt = select(ConfigModel).where(
        ConfigModel.key == request.key,
        ConfigModel.organization_id == org_uuid,
    )
    result = await db.execute(stmt)
    existing = result.scalar_one_or_none()

    if existing:
        existing.value = config_value
        existing.config_type = config_type
        existing.updated_at = now
        existing.updated_by = current_user.email
    else:
        config = ConfigModel(
            key=request.key,
            value=config_value,
            config_type=config_type,
            organization_id=org_uuid,
            created_at=now,
            updated_at=now,
            updated_by=current_user.email,
        )
        db.add(config)

    await db.commit()

    try:
        from src.core.cache import invalidate_config
        await invalidate_config(org_id, request.key)
    except ImportError:
        pass

    logger.info(f"CLI set config {request.key} for user {current_user.email}")


@router.post(
    "/config/list",
    summary="List config values",
)
async def cli_list_config(
    request: CLIConfigListRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> dict[str, Any]:
    """List all config values via CLI API."""
    org_id = await _get_cli_org_id(current_user.user_id, request.org_id, db)

    async with get_redis() as r:
        all_data = await r.hgetall(config_hash_key(org_id))  # type: ignore[misc]

        if not all_data:
            return {}

        config_dict: dict[str, Any] = {}
        for config_key, data in all_data.items():
            try:
                cache_entry = json.loads(data)
            except json.JSONDecodeError:
                continue

            raw_value = cache_entry.get("value")
            config_type = cache_entry.get("type", "string")

            if config_type == "secret":
                config_dict[config_key] = raw_value if raw_value else "[SECRET]"
            elif config_type == "json" and isinstance(raw_value, str):
                try:
                    config_dict[config_key] = json.loads(raw_value)
                except json.JSONDecodeError:
                    config_dict[config_key] = raw_value
            elif config_type == "bool":
                config_dict[config_key] = str(raw_value).lower() == "true" if isinstance(raw_value, str) else bool(raw_value)
            elif config_type == "int":
                try:
                    config_dict[config_key] = int(raw_value)
                except (ValueError, TypeError):
                    config_dict[config_key] = raw_value
            else:
                config_dict[config_key] = raw_value

        return config_dict


@router.post(
    "/config/delete",
    summary="Delete config value",
)
async def cli_delete_config(
    request: CLIConfigDeleteRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> bool:
    """Delete a config value via CLI API."""
    from src.models import Config as ConfigModel

    org_id = await _get_cli_org_id(current_user.user_id, request.org_id, db)
    org_uuid = UUID(org_id) if org_id else None

    stmt = select(ConfigModel).where(
        ConfigModel.key == request.key,
        ConfigModel.organization_id == org_uuid,
    )
    result = await db.execute(stmt)
    config = result.scalar_one_or_none()

    if not config:
        return False

    await db.delete(config)
    await db.commit()

    try:
        from src.core.cache import invalidate_config
        await invalidate_config(org_id, request.key)
    except ImportError:
        pass

    logger.info(f"CLI deleted config {request.key} for user {current_user.email}")
    return True


# =============================================================================
# SDK Integrations Endpoints
# =============================================================================


@router.post(
    "/integrations/get",
    response_model=SDKIntegrationsGetResponse | None,
    summary="Get integration data for an organization",
)
async def sdk_integrations_get(
    request: SDKIntegrationsGetRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> SDKIntegrationsGetResponse | None:
    """Get integration mapping data for an organization via SDK.

    Supports two modes:
    1. Org-specific mapping: Returns mapping entity_id, config, and OAuth data
    2. Fallback to integration defaults: When no org mapping exists, returns
       integration.default_entity_id, integration-level config, and OAuth data
    """
    from src.repositories.integrations import IntegrationsRepository
    from src.services.oauth_provider import resolve_url_template
    from src.core.security import decrypt_secret

    org_id = await _get_cli_org_id(current_user.user_id, request.org_id, db)
    org_uuid = UUID(org_id) if org_id else None

    try:
        repo = IntegrationsRepository(db)

        # Try to get org-specific mapping first
        mapping = None
        if org_uuid:
            mapping = await repo.get_integration_for_org(request.name, org_uuid)

        if mapping:
            # Org-specific mapping found
            config = await repo.get_config_for_mapping(mapping.integration_id, org_uuid)
            integration = mapping.integration
            entity_id = mapping.entity_id or (integration.default_entity_id if integration else None)

            response_data: dict[str, Any] = {
                "integration_id": str(mapping.integration_id),
                "entity_id": entity_id,
                "entity_name": mapping.entity_name,
                "config": config or {},
                "oauth": None,
            }

            # Build OAuth data if provider exists
            if integration and integration.oauth_provider:
                token = mapping.oauth_token
                if not token:
                    token = await repo.get_provider_org_token(integration.oauth_provider.id)
                response_data["oauth"] = await _build_oauth_data(
                    integration.oauth_provider, token, entity_id, resolve_url_template, decrypt_secret
                )

            logger.info(f"SDK retrieved integration '{request.name}' (org mapping) for user {current_user.email}")
            return SDKIntegrationsGetResponse(**response_data)

        # Fall back to integration defaults
        integration = await repo.get_integration_by_name(request.name)
        if not integration:
            logger.debug(f"SDK integrations.get('{request.name}'): integration not found")
            return None

        entity_id = integration.default_entity_id or integration.entity_id
        config = await repo.get_integration_defaults(integration.id)

        response_data = {
            "integration_id": str(integration.id),
            "entity_id": entity_id,
            "entity_name": None,  # No mapping = no entity name
            "config": config or {},
            "oauth": None,
        }

        # Build OAuth data if provider exists
        if integration.oauth_provider:
            token = await repo.get_provider_org_token(integration.oauth_provider.id)
            response_data["oauth"] = await _build_oauth_data(
                integration.oauth_provider, token, entity_id, resolve_url_template, decrypt_secret
            )

        logger.info(f"SDK retrieved integration '{request.name}' (defaults) for user {current_user.email}")
        return SDKIntegrationsGetResponse(**response_data)

    except Exception as e:
        logger.error(f"SDK integrations.get failed: {e}")
        return None


async def _build_oauth_data(
    provider: Any,
    token: Any,
    entity_id: str | None,
    resolve_url_template: Any,
    decrypt_secret: Any,
) -> SDKIntegrationsOAuthData:
    """Build OAuth data dict from provider and token for CLI response."""
    # Decrypt secrets
    client_secret = None
    if provider.encrypted_client_secret:
        try:
            raw = provider.encrypted_client_secret
            client_secret = await asyncio.to_thread(
                decrypt_secret, raw.decode() if isinstance(raw, bytes) else raw
            )
        except Exception:
            logger.warning("Failed to decrypt client_secret")

    access_token = None
    refresh_token = None
    expires_at = None

    if token:
        if token.encrypted_access_token:
            try:
                raw = token.encrypted_access_token
                access_token = await asyncio.to_thread(
                    decrypt_secret, raw.decode() if isinstance(raw, bytes) else raw
                )
            except Exception:
                logger.warning("Failed to decrypt access_token")

        if token.encrypted_refresh_token:
            try:
                raw = token.encrypted_refresh_token
                refresh_token = await asyncio.to_thread(
                    decrypt_secret, raw.decode() if isinstance(raw, bytes) else raw
                )
            except Exception:
                logger.warning("Failed to decrypt refresh_token")

        if token.expires_at:
            expires_at = token.expires_at.isoformat()

    # Resolve token_url with entity_id
    resolved_token_url = None
    if provider.token_url and entity_id:
        resolved_token_url = resolve_url_template(
            url=provider.token_url,
            entity_id=entity_id,
            defaults=provider.token_url_defaults,
        )

    return SDKIntegrationsOAuthData(
        connection_name=provider.provider_name,
        client_id=provider.client_id,
        client_secret=client_secret,
        authorization_url=provider.authorization_url,
        token_url=resolved_token_url,
        scopes=provider.scopes or [],
        access_token=access_token,
        refresh_token=refresh_token,
        expires_at=expires_at,
    )


@router.post(
    "/integrations/list_mappings",
    response_model=SDKIntegrationsListMappingsResponse | None,
    summary="List all mappings for an integration",
)
async def sdk_integrations_list_mappings(
    request: SDKIntegrationsListMappingsRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> SDKIntegrationsListMappingsResponse | None:
    """List all mappings for an integration via SDK."""
    from src.repositories.integrations import IntegrationsRepository

    try:
        repo = IntegrationsRepository(db)
        integration = await repo.get_integration_by_name(request.name)

        if not integration:
            logger.warning(f"SDK integrations.list_mappings: integration '{request.name}' not found")
            return None

        mappings = await repo.list_mappings(integration.id)

        logger.info(f"SDK listed {len(mappings)} mappings for integration '{request.name}' for user {current_user.email}")

        items = []
        for mapping in mappings:
            # Get merged config (integration defaults + org overrides)
            config = await repo.get_config_for_mapping(integration.id, mapping.organization_id)
            items.append({
                "id": str(mapping.id),
                "integration_id": str(mapping.integration_id),
                "organization_id": str(mapping.organization_id),
                "entity_id": mapping.entity_id,
                "entity_name": mapping.entity_name,
                "oauth_token_id": str(mapping.oauth_token_id) if mapping.oauth_token_id else None,
                "config": config,
                "created_at": mapping.created_at.isoformat(),
                "updated_at": mapping.updated_at.isoformat(),
            })

        return SDKIntegrationsListMappingsResponse(items=items)

    except Exception as e:
        logger.error(f"SDK integrations.list_mappings failed: {e}")
        return None


# =============================================================================
# CLI Session Endpoints (Database-backed)
# =============================================================================


@router.post(
    "/sessions",
    summary="Register/create a CLI session",
    response_model=CLISessionResponse,
)
async def register_cli_session(
    request: CLISessionRegisterRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> CLISessionResponse:
    """
    Register workflows discovered by CLI for web UI.

    Called by `bifrost run <file>` to register workflows before
    opening the browser to the CLI session page.
    """
    repo = CLISessionRepository(db)

    # Convert workflows to dict format for storage
    workflows_data = [
        {
            "name": w.name,
            "description": w.description,
            "parameters": [p.model_dump() if hasattr(p, 'model_dump') else p for p in w.parameters],
        }
        for w in request.workflows
    ]

    session = await repo.create_session(
        session_id=UUID(request.session_id),
        user_id=current_user.user_id,
        file_path=request.file_path,
        workflows=workflows_data,
        selected_workflow=request.selected_workflow,
    )
    await db.commit()

    logger.info(
        f"CLI session registered: {len(request.workflows)} workflows from {request.file_path} "
        f"for user {current_user.email}, session_id={request.session_id}"
    )

    response = _session_to_response(session, is_connected=True)

    # Broadcast state update via websocket
    await publish_cli_session_update(str(current_user.user_id), request.session_id, response.model_dump(mode="json"))

    return response


@router.get(
    "/sessions",
    summary="List user's CLI sessions",
    response_model=CLISessionListResponse,
)
async def list_cli_sessions(
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> CLISessionListResponse:
    """List all CLI sessions for the current user."""
    repo = CLISessionRepository(db)
    sessions = await repo.get_user_sessions(current_user.user_id)

    return CLISessionListResponse(
        sessions=[
            _session_to_response(s, is_connected=repo.is_connected(s))
            for s in sessions
        ]
    )


@router.get(
    "/sessions/{session_id}",
    summary="Get CLI session state",
    response_model=CLISessionResponse,
)
async def get_cli_session(
    session_id: str,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> CLISessionResponse:
    """Get current CLI session state for web UI."""
    repo = CLISessionRepository(db)

    try:
        session_uuid = UUID(session_id)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid session ID format",
        )

    session = await repo.get_session_for_user(session_uuid, current_user.user_id)

    if session is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Session not found",
        )

    return _session_to_response(session, is_connected=repo.is_connected(session))


@router.delete(
    "/sessions/{session_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Delete CLI session",
)
async def delete_cli_session(
    session_id: str,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> None:
    """Delete a CLI session."""
    repo = CLISessionRepository(db)

    try:
        session_uuid = UUID(session_id)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid session ID format",
        )

    session = await repo.get_session_for_user(session_uuid, current_user.user_id)

    if session is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Session not found",
        )

    await repo.delete(session)
    await db.commit()

    logger.info(f"CLI session deleted: {session_id} for user {current_user.email}")


@router.post(
    "/sessions/{session_id}/continue",
    summary="Continue workflow execution",
    status_code=status.HTTP_202_ACCEPTED,
    response_model=CLISessionContinueResponse,
)
async def continue_cli_session(
    session_id: str,
    request: CLISessionContinueRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> CLISessionContinueResponse:
    """
    Submit parameters to continue workflow execution.

    Called by web UI when user clicks "Continue".
    Creates a real Execution record and sets pending=True so CLI can pick up.
    """
    from src.repositories.executions import ExecutionRepository
    from src.models.enums import ExecutionStatus

    repo = CLISessionRepository(db)

    try:
        session_uuid = UUID(session_id)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid session ID format",
        )

    session = await repo.get_session_for_user(session_uuid, current_user.user_id)

    if session is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No active CLI session. Run `bifrost run <file>` first.",
        )

    # Validate workflow exists
    workflow_names = [w.get("name") for w in session.workflows]
    if request.workflow_name not in workflow_names:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Workflow '{request.workflow_name}' not found. Available: {workflow_names}",
        )

    # Create a real Execution record
    execution_id = str(uuid4())
    exec_repo = ExecutionRepository(db)

    await exec_repo.create_execution(
        execution_id=execution_id,
        workflow_name=request.workflow_name,
        parameters=request.params,
        org_id=str(current_user.organization_id) if current_user.organization_id else None,
        user_id=str(current_user.user_id),
        user_name=current_user.name or current_user.email,
        status=ExecutionStatus.PENDING,
        is_local_execution=True,
    )

    # Link execution to session
    from src.models.orm import Execution
    stmt = select(Execution).where(Execution.id == UUID(execution_id))
    result = await db.execute(stmt)
    execution = result.scalar_one_or_none()
    if execution:
        execution.session_id = session_uuid
        await db.flush()  # Ensure session_id is persisted for history page icon

    # Update session state
    await repo.set_pending(
        session_uuid,
        request.workflow_name,
        request.params,
    )
    await db.commit()

    logger.info(
        f"CLI session continue: workflow={request.workflow_name}, "
        f"execution_id={execution_id}, session_id={session_id}, user={current_user.email}"
    )

    # Broadcast state update via websocket
    updated_session = await repo.get_session_with_executions(session_uuid)
    if updated_session:
        response_data = _session_to_response(updated_session, is_connected=repo.is_connected(updated_session))
        await publish_cli_session_update(str(current_user.user_id), session_id, response_data.model_dump(mode="json"))

    return CLISessionContinueResponse(
        status="pending",
        execution_id=execution_id,
        workflow=request.workflow_name,
    )


@router.get(
    "/sessions/{session_id}/pending",
    summary="Poll for pending execution",
    response_model=CLISessionPendingResponse,
)
async def get_pending_execution(
    session_id: str,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> CLISessionPendingResponse:
    """
    Poll for pending workflow execution.

    Returns 204 No Content if no execution pending.
    Returns execution_id, params and clears pending flag when execution is ready.
    """
    from src.repositories.executions import ExecutionRepository
    from src.models.enums import ExecutionStatus

    repo = CLISessionRepository(db)

    try:
        session_uuid = UUID(session_id)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid session ID format",
        )

    session = await repo.get_session_for_user(session_uuid, current_user.user_id)

    if session is None or not session.pending:
        raise HTTPException(
            status_code=status.HTTP_204_NO_CONTENT,
            detail="No pending execution",
        )

    workflow_name = session.selected_workflow
    params = session.params

    if workflow_name is None or params is None:
        raise HTTPException(
            status_code=status.HTTP_204_NO_CONTENT,
            detail="No pending execution",
        )

    # Find the most recent pending execution for this session
    from src.models.orm import Execution
    stmt = select(Execution).where(
        Execution.session_id == session_uuid,
        Execution.status == ExecutionStatus.PENDING,
    ).order_by(Execution.created_at.desc()).limit(1)
    result = await db.execute(stmt)
    execution = result.scalar_one_or_none()

    if not execution:
        raise HTTPException(
            status_code=status.HTTP_204_NO_CONTENT,
            detail="No pending execution",
        )

    execution_id = str(execution.id)

    # Update execution status to RUNNING
    exec_repo = ExecutionRepository(db)
    await exec_repo.update_execution(
        execution_id=execution_id,
        status=ExecutionStatus.RUNNING,
    )

    # Clear pending and update last_seen
    await repo.clear_pending(session_uuid)
    await repo.update_last_seen(session_uuid)
    await db.commit()

    # Broadcast execution status update
    await publish_execution_update(execution_id, "Running")

    logger.info(f"CLI session pending picked up: workflow={workflow_name}, execution_id={execution_id}, session_id={session_id}")

    # Broadcast session state update
    updated_session = await repo.get_session_with_executions(session_uuid)
    if updated_session:
        response_data = _session_to_response(updated_session, is_connected=repo.is_connected(updated_session))
        await publish_cli_session_update(str(current_user.user_id), session_id, response_data.model_dump(mode="json"))

    return CLISessionPendingResponse(
        execution_id=execution_id,
        workflow_name=workflow_name,
        params=params,
    )


@router.post(
    "/sessions/{session_id}/heartbeat",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Update session heartbeat",
)
async def session_heartbeat(
    session_id: str,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> None:
    """Update session's last_seen timestamp (CLI heartbeat)."""
    repo = CLISessionRepository(db)

    try:
        session_uuid = UUID(session_id)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid session ID format",
        )

    session = await repo.get_session_for_user(session_uuid, current_user.user_id)
    if session:
        await repo.update_last_seen(session_uuid)
        await db.commit()


@router.post(
    "/sessions/{session_id}/executions/{execution_id}/log",
    summary="Stream log entry from CLI",
    status_code=status.HTTP_204_NO_CONTENT,
)
async def post_cli_log(
    session_id: str,
    execution_id: str,
    request: CLISessionLogRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> None:
    """Stream a log entry from CLI to the execution."""
    from src.models.orm import Execution

    try:
        exec_uuid = UUID(execution_id)
        session_uuid = UUID(session_id)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid ID format",
        )

    stmt = select(Execution).where(
        Execution.id == exec_uuid,
        Execution.session_id == session_uuid,
        Execution.executed_by == current_user.user_id,
        Execution.is_local_execution == True,  # noqa: E712
    )
    result = await db.execute(stmt)
    execution = result.scalar_one_or_none()

    if not execution:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Execution not found or not authorized",
        )

    timestamp = None
    if request.timestamp:
        try:
            # Parse timestamp and strip timezone to match engine behavior
            # (engine uses datetime.utcnow() which is timezone-naive)
            ts = datetime.fromisoformat(request.timestamp.replace("Z", "+00:00"))
            timestamp = ts.replace(tzinfo=None) if ts.tzinfo else ts
        except ValueError:
            timestamp = datetime.utcnow()

    try:
        # Use unified log function - same as workflow engine
        # Writes to Redis Stream (for persistence) AND publishes to PubSub (for WebSocket)
        from bifrost._logging import log_and_broadcast_async

        await log_and_broadcast_async(
            execution_id=execution_id,
            level=request.level,
            message=request.message,
            metadata=request.metadata,
            timestamp=timestamp,
        )
    except ImportError:
        logger.warning(f"Log streaming not available, log skipped: {request.message}")


@router.post(
    "/sessions/{session_id}/executions/{execution_id}/result",
    summary="Post execution result from CLI",
    status_code=status.HTTP_200_OK,
)
async def post_cli_result(
    session_id: str,
    execution_id: str,
    request: CLISessionResultRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> dict:
    """Post execution result from CLI."""
    from src.models.orm import Execution
    from src.models.enums import ExecutionStatus
    from src.repositories.executions import ExecutionRepository

    try:
        exec_uuid = UUID(execution_id)
        session_uuid = UUID(session_id)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid ID format",
        )

    stmt = select(Execution).where(
        Execution.id == exec_uuid,
        Execution.session_id == session_uuid,
        Execution.executed_by == current_user.user_id,
        Execution.is_local_execution == True,  # noqa: E712
    )
    result = await db.execute(stmt)
    execution = result.scalar_one_or_none()

    if not execution:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Execution not found or not authorized",
        )

    if request.status.lower() in ("success", "completed"):
        status_enum = ExecutionStatus.SUCCESS
    else:
        status_enum = ExecutionStatus.FAILED

    repo = ExecutionRepository(db)
    await repo.update_execution(
        execution_id=execution_id,
        status=status_enum,
        result=request.result,
        error_message=request.error_message,
        duration_ms=request.duration_ms,  # completed_at is set automatically when duration_ms is provided
    )

    # Persist logs directly from request (avoids race conditions)
    logs_persisted = 0
    if request.logs:
        from src.models.orm import ExecutionLog
        from datetime import datetime as dt

        logs_to_insert = []
        for seq, log in enumerate(request.logs):
            try:
                # Parse timestamp, strip timezone for DB
                if log.timestamp:
                    ts = dt.fromisoformat(log.timestamp)
                    if ts.tzinfo is not None:
                        ts = ts.replace(tzinfo=None)
                else:
                    ts = dt.utcnow()

                log_entry = ExecutionLog(
                    execution_id=exec_uuid,
                    level=log.level.upper(),
                    message=log.message,
                    log_metadata=log.metadata,
                    timestamp=ts,
                    sequence=seq,
                )
                logs_to_insert.append(log_entry)

                # Also broadcast to WebSocket for real-time UI update
                await publish_execution_log(
                    execution_id,
                    log.level,
                    log.message,
                    {"metadata": log.metadata, "timestamp": ts.isoformat()},
                )
            except Exception as e:
                logger.warning(f"Failed to process log entry: {e}")
                continue

        if logs_to_insert:
            db.add_all(logs_to_insert)
            logs_persisted = len(logs_to_insert)
            logger.debug(f"Persisted {logs_persisted} logs directly for CLI execution {execution_id}")

    await db.commit()

    # Fallback: flush any logs from Redis Stream (backwards compatibility)
    logs_flushed = 0
    if not request.logs:
        try:
            from bifrost._logging import flush_logs_to_postgres
            logs_flushed = await flush_logs_to_postgres(execution_id)
            if logs_flushed > 0:
                logger.debug(f"Flushed {logs_flushed} logs from stream for CLI execution {execution_id}")
        except ImportError:
            pass
        except Exception as e:
            logger.warning(f"Failed to flush logs from stream: {e}")

    # Match workflow engine format exactly for unified UI handling
    update_data: dict[str, Any] = {
        "result": request.result,
        "durationMs": request.duration_ms if request.duration_ms else 0,
    }
    if request.error_message:
        update_data["error"] = request.error_message

    await publish_execution_update(
        execution_id,
        status_enum.value,
        update_data,
    )

    # Broadcast updated session state
    session_repo = CLISessionRepository(db)
    updated_session = await session_repo.get_session_with_executions(session_uuid)
    if updated_session:
        response_data = _session_to_response(updated_session, is_connected=session_repo.is_connected(updated_session))
        await publish_cli_session_update(str(current_user.user_id), session_id, response_data.model_dump(mode="json"))

    total_logs = logs_persisted + logs_flushed
    logger.info(
        f"CLI result: execution_id={execution_id}, session_id={session_id}, status={status_enum.value}, "
        f"logs_persisted={logs_persisted}, logs_flushed={logs_flushed}, user={current_user.email}"
    )

    return {
        "status": status_enum.value,
        "logs_persisted": logs_persisted,
        "logs_flushed": logs_flushed,
        "total_logs": total_logs,
    }


# =============================================================================
# SDK AI Endpoints
# =============================================================================


@router.post(
    "/ai/complete",
    summary="Generate AI completion",
)
async def cli_ai_complete(
    request: "CLIAICompleteRequest",
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> "CLIAICompleteResponse":
    """Generate an AI completion using platform-configured LLM."""
    from src.models.contracts.cli import CLIAICompleteRequest, CLIAICompleteResponse
    from src.services.llm import get_llm_client, LLMMessage

    try:
        client = await get_llm_client(db)

        # Convert to LLMMessage objects
        llm_messages = [
            LLMMessage(role=msg["role"], content=msg["content"])  # type: ignore[arg-type]
            for msg in request.messages
        ]

        response = await client.complete(
            messages=llm_messages,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            model=request.model,
        )

        logger.info(f"CLI AI complete: model={response.model}, tokens={response.input_tokens}/{response.output_tokens}")

        return CLIAICompleteResponse(
            content=response.content,
            input_tokens=response.input_tokens,
            output_tokens=response.output_tokens,
            model=response.model,
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=str(e),
        )
    except Exception as e:
        logger.error(f"CLI AI complete failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"AI completion failed: {str(e)}",
        )


@router.post(
    "/ai/stream",
    summary="Stream AI completion",
)
async def cli_ai_stream(
    request: CLIAICompleteRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> StreamingResponse:
    """Generate a streaming AI completion using SSE."""
    from src.services.llm import get_llm_client, LLMMessage

    async def generate():
        try:
            client = await get_llm_client(db)

            # Convert to LLMMessage objects
            llm_messages = [
                LLMMessage(role=msg["role"], content=msg["content"])  # type: ignore[arg-type]
                for msg in request.messages
            ]

            async for chunk in client.stream(
                messages=llm_messages,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                model=request.model,
            ):
                if chunk.type == "delta":
                    yield f"data: {json.dumps({'content': chunk.content})}\n\n"
                elif chunk.type == "done":
                    yield f"data: {json.dumps({'done': True, 'input_tokens': chunk.input_tokens, 'output_tokens': chunk.output_tokens})}\n\n"
                    yield "data: [DONE]\n\n"
                elif chunk.type == "error":
                    yield f"data: {json.dumps({'error': chunk.error})}\n\n"
                    break
        except ValueError as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
        except Exception as e:
            logger.error(f"CLI AI stream failed: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        }
    )


@router.get(
    "/ai/info",
    summary="Get AI model information",
)
async def cli_ai_info(
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> "CLIAIInfoResponse":
    """Get information about the configured LLM."""
    from src.models.contracts.cli import CLIAIInfoResponse
    from src.services.llm.factory import get_llm_config

    try:
        config = await get_llm_config(db)

        return CLIAIInfoResponse(
            provider=config.provider,
            model=config.model,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=str(e),
        )


# =============================================================================
# SDK Knowledge Store Endpoints
# =============================================================================


@router.post(
    "/knowledge/store",
    summary="Store a document in knowledge store",
)
async def cli_knowledge_store(
    request: "CLIKnowledgeStoreRequest",
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> dict:
    """Store a document with its embedding in the knowledge store."""
    from src.models.contracts.cli import CLIKnowledgeStoreRequest
    from src.repositories.knowledge import KnowledgeRepository
    from src.services.embeddings import get_embedding_client

    try:
        org_id = await _get_cli_org_id(current_user.user_id, request.org_id, db)
        org_uuid = None
        if request.scope != "global" and org_id:
            org_uuid = UUID(org_id)

        # Generate embedding
        embedding_client = await get_embedding_client(db)
        embedding = await embedding_client.embed_single(request.content)

        # Store document
        repo = KnowledgeRepository(db)
        doc_id = await repo.store(
            content=request.content,
            embedding=embedding,
            namespace=request.namespace,
            key=request.key,
            metadata=request.metadata,
            organization_id=org_uuid,
            created_by=current_user.user_id,
        )

        await db.commit()

        logger.info(f"CLI knowledge store: namespace={request.namespace}, key={request.key}, doc_id={doc_id}")

        return {"id": doc_id}
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=str(e),
        )
    except Exception as e:
        logger.error(f"CLI knowledge store failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Knowledge store failed: {str(e)}",
        )


@router.post(
    "/knowledge/store-many",
    summary="Store multiple documents",
)
async def cli_knowledge_store_many(
    request: "CLIKnowledgeStoreManyRequest",
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> dict:
    """Store multiple documents with batch embedding."""
    from src.models.contracts.cli import CLIKnowledgeStoreManyRequest
    from src.repositories.knowledge import KnowledgeRepository
    from src.services.embeddings import get_embedding_client

    try:
        org_id = await _get_cli_org_id(current_user.user_id, request.org_id, db)
        org_uuid = None
        if request.scope != "global" and org_id:
            org_uuid = UUID(org_id)

        # Extract contents for batch embedding
        contents = [doc["content"] for doc in request.documents]

        # Batch generate embeddings
        embedding_client = await get_embedding_client(db)
        embeddings = await embedding_client.embed(contents)

        # Store each document
        repo = KnowledgeRepository(db)
        doc_ids = []
        for doc, embedding in zip(request.documents, embeddings):
            doc_id = await repo.store(
                content=doc["content"],
                embedding=embedding,
                namespace=request.namespace,
                key=doc.get("key"),
                metadata=doc.get("metadata"),
                organization_id=org_uuid,
                created_by=current_user.user_id,
            )
            doc_ids.append(doc_id)

        await db.commit()

        logger.info(f"CLI knowledge store-many: namespace={request.namespace}, count={len(doc_ids)}")

        return {"ids": doc_ids}
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=str(e),
        )
    except Exception as e:
        logger.error(f"CLI knowledge store-many failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Knowledge store failed: {str(e)}",
        )


@router.post(
    "/knowledge/search",
    summary="Search for similar documents",
)
async def cli_knowledge_search(
    request: "CLIKnowledgeSearchRequest",
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> list[CLIKnowledgeDocumentResponse]:
    """Search for similar documents using vector similarity."""
    from src.models.contracts.cli import CLIKnowledgeSearchRequest, CLIKnowledgeDocumentResponse
    from src.repositories.knowledge import KnowledgeRepository
    from src.services.embeddings import get_embedding_client

    try:
        org_id = await _get_cli_org_id(current_user.user_id, request.org_id, db)
        org_uuid = UUID(org_id) if org_id else None

        # Generate query embedding
        embedding_client = await get_embedding_client(db)
        query_embedding = await embedding_client.embed_single(request.query)

        # Search
        repo = KnowledgeRepository(db)
        results = await repo.search(
            query_embedding=query_embedding,
            namespace=request.namespace,
            organization_id=org_uuid,
            limit=request.limit,
            min_score=request.min_score,
            metadata_filter=request.metadata_filter,
            fallback=request.fallback,
        )

        logger.info(f"CLI knowledge search: query={request.query[:50]}..., results={len(results)}")

        return [
            CLIKnowledgeDocumentResponse(
                id=doc.id,
                namespace=doc.namespace,
                content=doc.content,
                metadata=doc.metadata,
                score=doc.score,
                organization_id=doc.organization_id,
                key=doc.key,
                created_at=doc.created_at.isoformat() if doc.created_at else None,
            )
            for doc in results
        ]
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=str(e),
        )
    except Exception as e:
        logger.error(f"CLI knowledge search failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Knowledge search failed: {str(e)}",
        )


@router.post(
    "/knowledge/delete",
    summary="Delete a document by key",
)
async def cli_knowledge_delete(
    request: "CLIKnowledgeDeleteRequest",
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
) -> dict:
    """Delete a document by key from the knowledge store."""
    from src.models.contracts.cli import CLIKnowledgeDeleteRequest
    from src.repositories.knowledge import KnowledgeRepository

    try:
        org_id = await _get_cli_org_id(current_user.user_id, request.org_id, db)
        org_uuid = None
        if request.scope != "global" and org_id:
            org_uuid = UUID(org_id)

        repo = KnowledgeRepository(db)
        deleted = await repo.delete_by_key(
            key=request.key,
            namespace=request.namespace,
            organization_id=org_uuid,
        )

        await db.commit()

        logger.info(f"CLI knowledge delete: namespace={request.namespace}, key={request.key}, deleted={deleted}")

        return {"deleted": deleted}
    except Exception as e:
        logger.error(f"CLI knowledge delete failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Knowledge delete failed: {str(e)}",
        )


@router.delete(
    "/knowledge/namespace/{namespace}",
    summary="Delete all documents in namespace",
)
async def cli_knowledge_delete_namespace(
    namespace: str,
    org_id: str | None = None,
    scope: str | None = None,
    current_user: CurrentUser = None,
    db: AsyncSession = Depends(get_db),
) -> dict:
    """Delete all documents in a namespace."""
    from src.repositories.knowledge import KnowledgeRepository

    try:
        resolved_org_id = await _get_cli_org_id(current_user.user_id, org_id, db)
        org_uuid = None
        if scope != "global" and resolved_org_id:
            org_uuid = UUID(resolved_org_id)

        repo = KnowledgeRepository(db)
        deleted_count = await repo.delete_namespace(
            namespace=namespace,
            organization_id=org_uuid,
        )

        await db.commit()

        logger.info(f"CLI knowledge delete namespace: namespace={namespace}, deleted_count={deleted_count}")

        return {"deleted_count": deleted_count}
    except Exception as e:
        logger.error(f"CLI knowledge delete namespace failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Knowledge delete namespace failed: {str(e)}",
        )


@router.get(
    "/knowledge/namespaces",
    summary="List namespaces with document counts",
)
async def cli_knowledge_list_namespaces(
    org_id: str | None = None,
    include_global: bool = True,
    current_user: CurrentUser = None,
    db: AsyncSession = Depends(get_db),
) -> list[CLIKnowledgeNamespaceInfo]:
    """List all namespaces with document counts per scope."""
    from src.models.contracts.cli import CLIKnowledgeNamespaceInfo
    from src.repositories.knowledge import KnowledgeRepository

    try:
        resolved_org_id = await _get_cli_org_id(current_user.user_id, org_id, db)
        org_uuid = UUID(resolved_org_id) if resolved_org_id else None

        repo = KnowledgeRepository(db)
        results = await repo.list_namespaces(
            organization_id=org_uuid,
            include_global=include_global,
        )

        return [
            CLIKnowledgeNamespaceInfo(
                namespace=ns.namespace,
                scopes=ns.scopes,
            )
            for ns in results
        ]
    except Exception as e:
        logger.error(f"CLI knowledge list namespaces failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Knowledge list namespaces failed: {str(e)}",
        )


@router.get(
    "/knowledge/get",
    summary="Get a document by key",
)
async def cli_knowledge_get(
    key: str,
    namespace: str = "default",
    org_id: str | None = None,
    scope: str | None = None,
    current_user: CurrentUser = None,
    db: AsyncSession = Depends(get_db),
) -> CLIKnowledgeDocumentResponse | None:
    """Get a document by key from the knowledge store."""
    from src.models.contracts.cli import CLIKnowledgeDocumentResponse
    from src.repositories.knowledge import KnowledgeRepository

    try:
        resolved_org_id = await _get_cli_org_id(current_user.user_id, org_id, db)
        org_uuid = None
        if scope != "global" and resolved_org_id:
            org_uuid = UUID(resolved_org_id)

        repo = KnowledgeRepository(db)
        result = await repo.get_by_key(
            key=key,
            namespace=namespace,
            organization_id=org_uuid,
        )

        if not result:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found",
            )

        return CLIKnowledgeDocumentResponse(
            id=result.id,
            namespace=result.namespace,
            content=result.content,
            metadata=result.metadata,
            organization_id=result.organization_id,
            key=result.key,
            created_at=result.created_at.isoformat() if result.created_at else None,
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"CLI knowledge get failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Knowledge get failed: {str(e)}",
        )


# =============================================================================
# CLI Download (generates installable package)
# =============================================================================


@router.get(
    "/download",
    summary="Download CLI package",
    description="Download the Bifrost CLI as a pip-installable tarball",
)
async def download_cli() -> StreamingResponse:
    """
    Serve CLI as installable package.

    Returns a tarball that can be installed with:
    pip install https://your-bifrost-instance.com/api/cli/download
    """
    package_dir = Path(__file__).parent.parent.parent / "bifrost"

    if not package_dir.exists():
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="CLI package not found",
        )

    buffer = io.BytesIO()

    # Files to exclude (platform-only internal files)
    exclude_files = {
        "_internal.py",     # Platform-only permission checks
        "_write_buffer.py", # Platform-only, requires Redis
        "_logging.py",      # Platform-only logging
        "_sync.py",         # Platform-only sync utilities
    }

    def _generate_tarball():
        """Generate tarball synchronously in thread."""
        with tarfile.open(fileobj=buffer, mode="w:gz") as tar:
            # Add pyproject.toml at root level
            pyproject_path = package_dir / "pyproject.toml"
            if pyproject_path.exists():
                tar.add(pyproject_path, arcname="pyproject.toml")

            # Add all Python files from bifrost/
            for file_path in package_dir.rglob("*"):
                if not file_path.is_file():
                    continue

                # Skip __pycache__ and excluded internal files
                if "__pycache__" in str(file_path):
                    continue
                if file_path.name in exclude_files:
                    continue
                # Skip non-Python files except pyproject.toml (already added at root)
                if file_path.suffix not in (".py", ".toml"):
                    continue
                if file_path.name == "pyproject.toml":
                    continue  # Already added at root

                # Include all other files
                arcname = f"bifrost/{file_path.relative_to(package_dir)}"
                tar.add(file_path, arcname=arcname)

    await asyncio.to_thread(_generate_tarball)
    buffer.seek(0)

    return StreamingResponse(
        buffer,
        media_type="application/gzip",
        headers={
            "Content-Disposition": "attachment; filename=bifrost-cli-2.0.0.tar.gz",
        },
    )
